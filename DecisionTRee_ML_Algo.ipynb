{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOu8FKYUR01Yn0a8jYtm/id"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Question 1: What is a Decision Tree, and how does it work in the context of classification?\n","\n","  - Definition:\n","A Decision Tree is a supervised machine learning algorithm used for both classification and regression tasks.\n","In the context of classification, it works by splitting the dataset into smaller subsets based on feature values, forming a tree-like structure of decisions that leads to class labels.\n","\n","     How it works (Step by Step for Classification):\n","\n","1.Root Node Selection\n","\n","- The process starts at the root node (the top of the tree).\n","\n","- The algorithm chooses the best feature to split the data using criteria such as:\n","\n","- Gini Impurity\n","\n","- Entropy / Information Gain\n","\n","- Chi-Square\n","\n","2. Splitting\n","\n","- The chosen feature splits the dataset into branches (subgroups).\n","\n","- Each branch represents a decision based on the feature value.\n","\n","3.Recursive Partitioning\n","\n","- For each branch, the algorithm again selects the best feature to further split the data.\n","\n","- This process continues recursively until:\n","\n","- A stopping criterion is met (e.g., max depth, min samples per leaf).\n","\n","- Or all samples in a node belong to the same class (pure node).\n","\n","4. Leaf Nodes (Prediction)\n","\n","- The final nodes (leaves) contain the class label prediction.\n","\n","- For classification:\n","\n","- If a leaf has 80% \"Yes\" and 20% \"No\" samples, the model predicts \"Yes\" for new data falling into that leaf.\n","\n","- Example (Binary Classification):\n","\n","-  Suppose we want to classify whether a person will buy a computer.\n","\n","     Root Node: \"Age\"\n","\n","    If Age < 30 → Check \"Income\"\n","\n","    If Age 30–50 → Predict \"Yes\"\n","\n","     If Age > 50 → Check \"Student\"\n","\n","This process continues until we reach class labels (\"Yes\" or \"No\").\n","\n","Advantages:\n","\n","-  Easy to understand and interpret (like a flowchart).\n","\n","-  Handles both numerical and categorical data.\n","\n","-  Requires little data preprocessing (no scaling needed).\n","\n","Limitations:\n","\n","- Can easily overfit if not pruned or limited.\n","\n","-  Sensitive to small changes in data.\n","\n","-  Bias toward features with many levels."],"metadata":{"id":"QlomFmL0Qu-5"}},{"cell_type":"markdown","source":["Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n","\n","- 1. Pre-Pruning (Early Stopping)\n","\n","  Definition: Pre-pruning stops the tree from growing too deep while it is being built.\n","\n","   It applies constraints during tree construction to avoid overly complex trees.\n","\n","  Common pre-pruning techniques:\n","\n","  Limit maximum depth (max_depth).\n","\n","  Minimum samples required to split a node (min_samples_split).\n","\n","  Minimum samples per leaf (min_samples_leaf).\n","\n","  Maximum number of leaf nodes (max_leaf_nodes).\n","\n","  Stop splitting if impurity improvement is below a threshold (min_impurity_decrease).\n","\n","- Practical Advantage:\n"," Efficiency: Saves time and memory by preventing unnecessary tree growth.\n","Example: Useful when dealing with very large datasets where training speed matters.\n","\n","2. Post-Pruning (Pruning After Full Growth)\n","\n","-  Definition: Post-pruning allows the tree to grow fully and then removes branches that do not improve model performance significantly.\n","\n","   The idea is: start complex → simplify.\n","\n","   Common post-pruning techniques:\n","\n","   Reduced Error Pruning: Replace a subtree with a leaf if it doesn’t worsen accuracy on validation data.\n","\n","   Cost Complexity Pruning (used in CART): Prune nodes that provide the least benefit relative to their complexity.\n","Practical Advantage:\n","Better Generalization: By pruning after full growth, we can analyze the tree structure and remove branches that cause overfitting.\n","Example: Useful when interpretability and accuracy on unseen data are more important than speed."],"metadata":{"id":"dOkXu-mUVLHn"}},{"cell_type":"markdown","source":["Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?● Iris Dataset for classification tasks (sklearn.datasets.load_iris() or provided CSV). ● Boston Housing Dataset for regression tasks (sklearn.datasets.load_boston() or provided CSV)\n","\n"," - 1. Real-World Applications\n","\n","     (a) Classification\n","\n","   Iris Dataset (Classic Example in ML):\n","\n","     Task: Classify iris flowers into species (Setosa, Versicolor, Virginica) based on features like petal length, sepal width, etc.\n","\n","Dataset: sklearn.datasets.load_iris()\n","\n","  Use Case: Demonstrates how Decision Trees can handle multi-class classification.\n","\n","  Other Real-World Classification Examples:\n","\n","  Medical Diagnosis: Classifying patients as “disease” or “no disease”.\n","\n","  Fraud Detection: Detecting fraudulent vs. legitimate transactions.\n","\n","Customer Churn Prediction: Predicting if a customer will leave a service.\n","\n","(b) Regression\n","\n","Boston Housing Dataset (Classic Example in Regression):\n","\n","-  Task: Predict median house prices based on features like crime rate, number of rooms, distance to employment centers, etc.\n","\n","-  Dataset: sklearn.datasets.load_boston() (deprecated, but still classic) or modern alternatives like California Housing dataset.\n","\n","-  Use Case: Demonstrates how Decision Trees can predict continuous outcomes.\n","\n","-  Other Real-World Regression Examples:\n","\n","-  Predicting sales based on advertisement spending.\n","\n","-  Forecasting demand for energy or products.\n","\n","-  Estimating credit risk score.\n","\n","2. Advantages of Decision Trees\n","\n","-  Easy to Interpret & Visualize\n","\n","-  Looks like a flowchart → business/non-technical stakeholders can understand.\n","\n","-  No Need for Feature Scaling\n","\n","-  Unlike SVM or Logistic Regression, no normalization/standardization required.\n","\n","-  Handles Both Categorical & Numerical Data\n","\n","-  Works with mixed data types.\n","\n","-  Captures Non-linear Relationships\n","\n","-  Can model complex decision boundaries.\n","\n","- Fast Inference\n","\n","-  Once trained, predictions are quick (just follow tree branches).\n","\n","3. Limitations of Decision Trees\n","\n","-  Overfitting\n","\n","-  Trees can grow too deep → memorizing noise in training data.\n","\n","-   (Needs pruning or ensemble methods like Random Forest).\n","\n","Unstable\n","\n","-  Small changes in data can produce very different trees.\n","\n","-  Biased Toward Features with More Levels\n","\n","-  Features with many unique values (like IDs) can dominate splits.\n","\n","-  Not Great at Extrapolation in Regression\n","\n","-  If values outside training range appear, tree can’t predict well.\n","\n","- Less Accurate Alone\n","\n","   Often outperformed by ensemble models (Random Forest, XGBoost, LightGBM).\n","\n","   "],"metadata":{"id":"QeWVoCjRW2CY"}},{"cell_type":"markdown","source":["Question 6: Write a Python program to:\n","\n","- Load the Iris Dataset\n","\n","- Train a Decision Tree Classifier using the Gini criterion\n","\n","-  Print the model’s accuracy and feature importances\n"],"metadata":{"id":"KFwYPbm5Z2Pl"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# 2. Split into training and test sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# 3. Train Decision Tree Classifier using Gini criterion\n","clf = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n","clf.fit(X_train, y_train)\n","\n","# 4. Evaluate the model\n","y_pred = clf.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# 5. Print results\n","print(\"Decision Tree Classifier (Gini Criterion)\")\n","print(f\"Accuracy on test set: {accuracy:.2f}\")\n","\n","# Feature importances\n","print(\"\\nFeature Importances:\")\n","for feature_name, importance in zip(iris.feature_names, clf.feature_importances_):\n","    print(f\"{feature_name}: {importance:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"k0pU1Sr3aBT5","executionInfo":{"status":"ok","timestamp":1758040988564,"user_tz":-330,"elapsed":2815,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"221fc64d-a413-406f-f2b9-60b9376e5259"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Classifier (Gini Criterion)\n","Accuracy on test set: 0.93\n","\n","Feature Importances:\n","sepal length (cm): 0.006\n","sepal width (cm): 0.029\n","petal length (cm): 0.559\n","petal width (cm): 0.406\n"]}]},{"cell_type":"markdown","source":["Question 7: Write a Python program to: ● Load the Iris Dataset ● Train a Decision Tree Classifier with max_depth=3 and compare its accuracy to a fully-grown tree."],"metadata":{"id":"BGNUiVIgaSXO"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# 2. Split into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# 3. Train a fully-grown Decision Tree\n","full_tree = DecisionTreeClassifier(random_state=42)  # no depth limit\n","full_tree.fit(X_train, y_train)\n","y_pred_full = full_tree.predict(X_test)\n","accuracy_full = accuracy_score(y_test, y_pred_full)\n","\n","# 4. Train a Decision Tree with max_depth = 3 (pre-pruned)\n","shallow_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n","shallow_tree.fit(X_train, y_train)\n","y_pred_shallow = shallow_tree.predict(X_test)\n","accuracy_shallow = accuracy_score(y_test, y_pred_shallow)\n","\n","# 5. Print Results\n","print(\"Decision Tree Accuracy Comparison\")\n","print(f\"Fully-Grown Tree Accuracy : {accuracy_full:.2f}\")\n","print(f\"Max Depth = 3 Tree Accuracy: {accuracy_shallow:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q324eghwaTrs","executionInfo":{"status":"ok","timestamp":1758041045995,"user_tz":-330,"elapsed":515,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"498dd851-8fa5-4ec8-a220-81d592384a5a"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Accuracy Comparison\n","Fully-Grown Tree Accuracy : 0.93\n","Max Depth = 3 Tree Accuracy: 0.97\n"]}]},{"cell_type":"markdown","source":["Question 8: Write a Python program to: ● Load the California Housing dataset from sklearn ● Train a Decision Tree Regressor ● Print the Mean Squared Error (MSE) and feature importances"],"metadata":{"id":"oZheufZPadEf"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","\n","# 1. Load the California Housing dataset\n","housing = fetch_california_housing()\n","X, y = housing.data, housing.target\n","\n","# 2. Split into training and test sets (80/20 split)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42\n",")\n","\n","# 3. Train Decision Tree Regressor\n","regressor = DecisionTreeRegressor(random_state=42)\n","regressor.fit(X_train, y_train)\n","\n","# 4. Predictions and Evaluation\n","y_pred = regressor.predict(X_test)\n","mse = mean_squared_error(y_test, y_pred)\n","\n","# 5. Print results\n","print(\"Decision Tree Regressor on California Housing\")\n","print(f\"Mean Squared Error (MSE): {mse:.2f}\")\n","\n","print(\"\\nFeature Importances:\")\n","for feature_name, importance in zip(housing.feature_names, regressor.feature_importances_):\n","    print(f\"{feature_name}: {importance:.3f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xGMoD2wHaeNu","executionInfo":{"status":"ok","timestamp":1758041091887,"user_tz":-330,"elapsed":2867,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"881e379e-165e-42ab-e019-9f46278297e2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Decision Tree Regressor on California Housing\n","Mean Squared Error (MSE): 0.50\n","\n","Feature Importances:\n","MedInc: 0.529\n","HouseAge: 0.052\n","AveRooms: 0.053\n","AveBedrms: 0.029\n","Population: 0.031\n","AveOccup: 0.131\n","Latitude: 0.094\n","Longitude: 0.083\n"]}]},{"cell_type":"markdown","source":["Question 9: Write a Python program to: ● Load the Iris Dataset ● Tune the Decision Tree’s max_depth and min_samples_split using GridSearchCV ● Print the best parameters and the resulting model accuracy"],"metadata":{"id":"5SBFy5hFan4A"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","# 1. Load the Iris dataset\n","iris = load_iris()\n","X, y = iris.data, iris.target\n","\n","# 2. Split into training and test sets (80/20)\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.2, random_state=42, stratify=y\n",")\n","\n","# 3. Define the Decision Tree Classifier\n","dt = DecisionTreeClassifier(random_state=42)\n","\n","# 4. Define the hyperparameter grid\n","param_grid = {\n","    \"max_depth\": [2, 3, 4, 5, None],\n","    \"min_samples_split\": [2, 3, 4, 5, 10]\n","}\n","\n","# 5. GridSearchCV for hyperparameter tuning\n","grid_search = GridSearchCV(\n","    estimator=dt,\n","    param_grid=param_grid,\n","    cv=5,                # 5-fold cross-validation\n","    scoring=\"accuracy\",\n","    n_jobs=-1\n",")\n","\n","grid_search.fit(X_train, y_train)\n","\n","# 6. Get best parameters and evaluate on test set\n","best_params = grid_search.best_params_\n","best_model = grid_search.best_estimator_\n","\n","y_pred = best_model.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","\n","# 7. Print results\n","print(\"Best Parameters:\", best_params)\n","print(f\"Accuracy of best model on test set: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f7GWfeJbapE_","executionInfo":{"status":"ok","timestamp":1758041145070,"user_tz":-330,"elapsed":5224,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"400a9da2-7640-4325-bcdb-3defc8807b2c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Best Parameters: {'max_depth': 4, 'min_samples_split': 2}\n","Accuracy of best model on test set: 0.93\n"]}]},{"cell_type":"markdown","source":["Question 10: Imagine you’re working as a data scientist for a healthcare company that\n","wants to predict whether a patient has a certain disease. You have a large dataset with\n","mixed data types and some missing values.\n","Explain the step-by-step process you would follow to:\n","\n","- Handle the missing values\n","-  Encode the categorical features\n","-  Train a Decision Tree model\n","-  Tune its hyperparameters\n","-    Evaluate its performanceAnd describe what business value this model could provide in the real-world setting."],"metadata":{"id":"PvtbDlfqa6iI"}},{"cell_type":"markdown","source":["tep 1: Handle Missing Values\n","\n","Why: Medical datasets often have missing entries (e.g., missing lab results, unrecorded symptoms). Decision Trees can handle some missing values, but preprocessing improves reliability.\n","\n","Approach:\n","\n","1 Numerical features → Impute with:\n","- Mean/median (if data is normally distributed/skewed).\n","- More advanced: KNN imputation or regression imputation.\n","\n","2 Categorical features → Impute with:\n","\n","- Mode (most frequent value).\n","\n","- Or create a special category like \"Unknown\""],"metadata":{"id":"YizioPzYbe1s"}},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer\n","\n","num_imputer = SimpleImputer(strategy=\"median\")\n","cat_imputer = SimpleImputer(strategy=\"most_frequent\")"],"metadata":{"id":"V1UoBaUucYC2","executionInfo":{"status":"ok","timestamp":1758041589988,"user_tz":-330,"elapsed":8,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["Step 2: Encode Categorical Features\n","\n","- Decision Trees don’t need feature scaling, but they do need numerical inputs.\n","\n","- Encoding approaches:\n","\n","- One-Hot Encoding: For nominal categories (e.g., blood type A/B/O).\n","\n","- Ordinal Encoding: For ordered categories (e.g., disease stage I/II/III)."],"metadata":{"id":"SFv4GVXQcigX"}},{"cell_type":"code","source":["from sklearn.preprocessing import OneHotEncoder"],"metadata":{"id":"JYSe7cwrcyyk","executionInfo":{"status":"ok","timestamp":1758041699095,"user_tz":-330,"elapsed":649,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Step 3: Train a Decision Tree Model\n","\n","Split dataset into train/test (e.g., 80/20).\n","\n","Train using DecisionTreeClassifier (criterion = \"gini\" or \"entropy\")."],"metadata":{"id":"cPL2kC_Wc4DK"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","clf = DecisionTreeClassifier(random_state=42)\n","clf.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":80},"id":"7o23fkzddHWr","executionInfo":{"status":"ok","timestamp":1758041779262,"user_tz":-330,"elapsed":19,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"4e770b8d-1eac-4af6-b94c-eb6f2ec2d20a"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(random_state=42)"],"text/html":["<style>#sk-container-id-1 {\n","  /* Definition of color scheme common for light and dark mode */\n","  --sklearn-color-text: #000;\n","  --sklearn-color-text-muted: #666;\n","  --sklearn-color-line: gray;\n","  /* Definition of color scheme for unfitted estimators */\n","  --sklearn-color-unfitted-level-0: #fff5e6;\n","  --sklearn-color-unfitted-level-1: #f6e4d2;\n","  --sklearn-color-unfitted-level-2: #ffe0b3;\n","  --sklearn-color-unfitted-level-3: chocolate;\n","  /* Definition of color scheme for fitted estimators */\n","  --sklearn-color-fitted-level-0: #f0f8ff;\n","  --sklearn-color-fitted-level-1: #d4ebff;\n","  --sklearn-color-fitted-level-2: #b3dbfd;\n","  --sklearn-color-fitted-level-3: cornflowerblue;\n","\n","  /* Specific color for light theme */\n","  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n","  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n","  --sklearn-color-icon: #696969;\n","\n","  @media (prefers-color-scheme: dark) {\n","    /* Redefinition of color scheme for dark theme */\n","    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n","    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n","    --sklearn-color-icon: #878787;\n","  }\n","}\n","\n","#sk-container-id-1 {\n","  color: var(--sklearn-color-text);\n","}\n","\n","#sk-container-id-1 pre {\n","  padding: 0;\n","}\n","\n","#sk-container-id-1 input.sk-hidden--visually {\n","  border: 0;\n","  clip: rect(1px 1px 1px 1px);\n","  clip: rect(1px, 1px, 1px, 1px);\n","  height: 1px;\n","  margin: -1px;\n","  overflow: hidden;\n","  padding: 0;\n","  position: absolute;\n","  width: 1px;\n","}\n","\n","#sk-container-id-1 div.sk-dashed-wrapped {\n","  border: 1px dashed var(--sklearn-color-line);\n","  margin: 0 0.4em 0.5em 0.4em;\n","  box-sizing: border-box;\n","  padding-bottom: 0.4em;\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","#sk-container-id-1 div.sk-container {\n","  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n","     but bootstrap.min.css set `[hidden] { display: none !important; }`\n","     so we also need the `!important` here to be able to override the\n","     default hidden behavior on the sphinx rendered scikit-learn.org.\n","     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n","  display: inline-block !important;\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-text-repr-fallback {\n","  display: none;\n","}\n","\n","div.sk-parallel-item,\n","div.sk-serial,\n","div.sk-item {\n","  /* draw centered vertical line to link estimators */\n","  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n","  background-size: 2px 100%;\n","  background-repeat: no-repeat;\n","  background-position: center center;\n","}\n","\n","/* Parallel-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-parallel-item::after {\n","  content: \"\";\n","  width: 100%;\n","  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n","  flex-grow: 1;\n","}\n","\n","#sk-container-id-1 div.sk-parallel {\n","  display: flex;\n","  align-items: stretch;\n","  justify-content: center;\n","  background-color: var(--sklearn-color-background);\n","  position: relative;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item {\n","  display: flex;\n","  flex-direction: column;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:first-child::after {\n","  align-self: flex-end;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:last-child::after {\n","  align-self: flex-start;\n","  width: 50%;\n","}\n","\n","#sk-container-id-1 div.sk-parallel-item:only-child::after {\n","  width: 0;\n","}\n","\n","/* Serial-specific style estimator block */\n","\n","#sk-container-id-1 div.sk-serial {\n","  display: flex;\n","  flex-direction: column;\n","  align-items: center;\n","  background-color: var(--sklearn-color-background);\n","  padding-right: 1em;\n","  padding-left: 1em;\n","}\n","\n","\n","/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n","clickable and can be expanded/collapsed.\n","- Pipeline and ColumnTransformer use this feature and define the default style\n","- Estimators will overwrite some part of the style using the `sk-estimator` class\n","*/\n","\n","/* Pipeline and ColumnTransformer style (default) */\n","\n","#sk-container-id-1 div.sk-toggleable {\n","  /* Default theme specific background. It is overwritten whether we have a\n","  specific estimator or a Pipeline/ColumnTransformer */\n","  background-color: var(--sklearn-color-background);\n","}\n","\n","/* Toggleable label */\n","#sk-container-id-1 label.sk-toggleable__label {\n","  cursor: pointer;\n","  display: flex;\n","  width: 100%;\n","  margin-bottom: 0;\n","  padding: 0.5em;\n","  box-sizing: border-box;\n","  text-align: center;\n","  align-items: start;\n","  justify-content: space-between;\n","  gap: 0.5em;\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label .caption {\n","  font-size: 0.6rem;\n","  font-weight: lighter;\n","  color: var(--sklearn-color-text-muted);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n","  /* Arrow on the left of the label */\n","  content: \"▸\";\n","  float: left;\n","  margin-right: 0.25em;\n","  color: var(--sklearn-color-icon);\n","}\n","\n","#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n","  color: var(--sklearn-color-text);\n","}\n","\n","/* Toggleable content - dropdown */\n","\n","#sk-container-id-1 div.sk-toggleable__content {\n","  max-height: 0;\n","  max-width: 0;\n","  overflow: hidden;\n","  text-align: left;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content pre {\n","  margin: 0.2em;\n","  border-radius: 0.25em;\n","  color: var(--sklearn-color-text);\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n","  /* Expand drop-down */\n","  max-height: 200px;\n","  max-width: 100%;\n","  overflow: auto;\n","}\n","\n","#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n","  content: \"▾\";\n","}\n","\n","/* Pipeline/ColumnTransformer-specific style */\n","\n","#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator-specific style */\n","\n","/* Colorize estimator box */\n","#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n","#sk-container-id-1 div.sk-label label {\n","  /* The background is the default theme color */\n","  color: var(--sklearn-color-text-on-default-background);\n","}\n","\n","/* On hover, darken the color of the background */\n","#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","/* Label box, darken color on hover, fitted */\n","#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n","  color: var(--sklearn-color-text);\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Estimator label */\n","\n","#sk-container-id-1 div.sk-label label {\n","  font-family: monospace;\n","  font-weight: bold;\n","  display: inline-block;\n","  line-height: 1.2em;\n","}\n","\n","#sk-container-id-1 div.sk-label-container {\n","  text-align: center;\n","}\n","\n","/* Estimator-specific */\n","#sk-container-id-1 div.sk-estimator {\n","  font-family: monospace;\n","  border: 1px dotted var(--sklearn-color-border-box);\n","  border-radius: 0.25em;\n","  box-sizing: border-box;\n","  margin-bottom: 0.5em;\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-0);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-0);\n","}\n","\n","/* on hover */\n","#sk-container-id-1 div.sk-estimator:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-2);\n","}\n","\n","#sk-container-id-1 div.sk-estimator.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-2);\n","}\n","\n","/* Specification for estimator info (e.g. \"i\" and \"?\") */\n","\n","/* Common style for \"i\" and \"?\" */\n","\n",".sk-estimator-doc-link,\n","a:link.sk-estimator-doc-link,\n","a:visited.sk-estimator-doc-link {\n","  float: right;\n","  font-size: smaller;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1em;\n","  height: 1em;\n","  width: 1em;\n","  text-decoration: none !important;\n","  margin-left: 0.5em;\n","  text-align: center;\n","  /* unfitted */\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-unfitted-level-1);\n","}\n","\n",".sk-estimator-doc-link.fitted,\n","a:link.sk-estimator-doc-link.fitted,\n","a:visited.sk-estimator-doc-link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",".sk-estimator-doc-link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover,\n","div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",".sk-estimator-doc-link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","/* Span, style for the box shown on hovering the info icon */\n",".sk-estimator-doc-link span {\n","  display: none;\n","  z-index: 9999;\n","  position: relative;\n","  font-weight: normal;\n","  right: .2ex;\n","  padding: .5ex;\n","  margin: .5ex;\n","  width: min-content;\n","  min-width: 20ex;\n","  max-width: 50ex;\n","  color: var(--sklearn-color-text);\n","  box-shadow: 2pt 2pt 4pt #999;\n","  /* unfitted */\n","  background: var(--sklearn-color-unfitted-level-0);\n","  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n","}\n","\n",".sk-estimator-doc-link.fitted span {\n","  /* fitted */\n","  background: var(--sklearn-color-fitted-level-0);\n","  border: var(--sklearn-color-fitted-level-3);\n","}\n","\n",".sk-estimator-doc-link:hover span {\n","  display: block;\n","}\n","\n","/* \"?\"-specific style due to the `<a>` HTML tag */\n","\n","#sk-container-id-1 a.estimator_doc_link {\n","  float: right;\n","  font-size: 1rem;\n","  line-height: 1em;\n","  font-family: monospace;\n","  background-color: var(--sklearn-color-background);\n","  border-radius: 1rem;\n","  height: 1rem;\n","  width: 1rem;\n","  text-decoration: none;\n","  /* unfitted */\n","  color: var(--sklearn-color-unfitted-level-1);\n","  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted {\n","  /* fitted */\n","  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n","  color: var(--sklearn-color-fitted-level-1);\n","}\n","\n","/* On hover */\n","#sk-container-id-1 a.estimator_doc_link:hover {\n","  /* unfitted */\n","  background-color: var(--sklearn-color-unfitted-level-3);\n","  color: var(--sklearn-color-background);\n","  text-decoration: none;\n","}\n","\n","#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n","  /* fitted */\n","  background-color: var(--sklearn-color-fitted-level-3);\n","}\n","</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow\"><div><div>DecisionTreeClassifier</div></div><div><a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.6/modules/generated/sklearn.tree.DecisionTreeClassifier.html\">?<span>Documentation for DecisionTreeClassifier</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></div></label><div class=\"sk-toggleable__content fitted\"><pre>DecisionTreeClassifier(random_state=42)</pre></div> </div></div></div></div>"]},"metadata":{},"execution_count":7}]},{"cell_type":"markdown","source":["Step 4: Hyperparameter Tuning\n","\n","Prevents overfitting. Use GridSearchCV to find the best parameters:\n","\n","max_depth → controls tree depth.\n","\n","min_samples_split / min_samples_leaf → control minimum samples required for splits.\n","\n","criterion → \"gini\" or \"entropy\""],"metadata":{"id":"CAyPhzAfdRRH"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\n","    \"max_depth\": [3, 5, 10, None],\n","    \"min_samples_split\": [2, 5, 10],\n","    \"min_samples_leaf\": [1, 2, 4],\n","    \"criterion\": [\"gini\", \"entropy\"]\n","}\n","grid = GridSearchCV(clf, param_grid, cv=5, scoring=\"accuracy\", n_jobs=-1)\n","grid.fit(X_train, y_train)\n","best_model = grid.best_estimator_\n"],"metadata":{"id":"oWyHURy3dSaF","executionInfo":{"status":"ok","timestamp":1758041829793,"user_tz":-330,"elapsed":3989,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["Step 5: Model Evaluation\n","\n","Metrics to use (Healthcare context):\n","\n","Accuracy: Good, but not enough (especially with imbalanced data).\n","\n","Precision & Recall: Important — recall ensures we don’t miss actual patients with disease.\n","\n","F1-score: Balance between precision & recall.\n","\n","ROC-AUC: Measures model’s ability to distinguish disease vs. no disease."],"metadata":{"id":"VWi6t-z6dXQP"}},{"cell_type":"code","source":["from sklearn.metrics import classification_report, roc_auc_score\n","\n","y_pred = best_model.predict(X_test)\n","print(classification_report(y_test, y_pred))\n","print(\"ROC-AUC:\", roc_auc_score(y_test, best_model.predict_proba(X_test)[:,1]))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":495},"id":"wWeSUF4QdeL0","executionInfo":{"status":"error","timestamp":1758042009190,"user_tz":-330,"elapsed":602,"user":{"displayName":"Gourisankar Maity","userId":"01828907568284795740"}},"outputId":"f2c16a35-03b3-488c-f36c-ba7caf2f13cb"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00        10\n","           1       0.90      0.90      0.90        10\n","           2       0.90      0.90      0.90        10\n","\n","    accuracy                           0.93        30\n","   macro avg       0.93      0.93      0.93        30\n","weighted avg       0.93      0.93      0.93        30\n","\n"]},{"output_type":"error","ename":"ValueError","evalue":"multi_class must be in ('ovo', 'ovr')","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1201486156.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ROC-AUC:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_auc_score\u001b[0;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[1;32m    632\u001b[0m             )\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmulti_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"multi_class must be in ('ovo', 'ovr')\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         return _multiclass_roc_auc_score(\n\u001b[1;32m    636\u001b[0m             \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmulti_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"]}]},{"cell_type":"markdown","source":["Step 6: Business Value in Real-World Setting\n","\n","Early Disease Detection: Helps doctors identify patients at risk → early treatment → improved outcomes.\n","\n","Resource Allocation: Hospitals can prioritize high-risk patients (e.g., ICU beds, specialist referrals).\n","\n","Cost Savings: Reduces unnecessary tests for low-risk patients while focusing on critical cases.\n","\n","Decision Support: Provides interpretable rules (why a patient is classified as “at risk”), which doctors trust more than black-box models."],"metadata":{"id":"9HWuq94md2LX"}},{"cell_type":"markdown","source":[],"metadata":{"id":"be0eNXoNhwxG"}}]}